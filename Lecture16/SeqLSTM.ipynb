{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import spacy\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/Text/SentimentAnalysis/Positive.txt')\n",
    "pos = f.read()\n",
    "f.close()\n",
    "\n",
    "f = open('../data/Text/SentimentAnalysis/Negative.txt')\n",
    "neg = f.read()\n",
    "f.close()\n",
    "#print pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy word embeddings\n",
    "word_embeddings = spacy.load('en', vectors='glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "(1, 20, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create a function to get vector format data for a sequence\n",
    "def sequence_to_data(seq, max_len=None):\n",
    "    seq = unicode(seq)\n",
    "    data = [word_embeddings(ix).vector for ix in seq.split()]\n",
    "    print len(data[3])\n",
    "    if max_len is None:\n",
    "        max_len = len(data)\n",
    "    \n",
    "    data_mat = np.zeros((1, max_len, 384))\n",
    "    \n",
    "    for ix in range(min(len(data), max_len)):\n",
    "        \n",
    "        data_mat[:, ix, :] = data[ix]\n",
    "    \n",
    "    return data_mat\n",
    "\n",
    "def seq_data_matrix(seq_data, max_len=None):\n",
    "    data = np.concatenate([sequence_to_data(ix, max_len) for ix in seq_data], axis=0)\n",
    "    print data.shape\n",
    "    return data\n",
    "    \n",
    "q = sequence_to_data(u'hello! what is the date today?', 20)\n",
    "\n",
    "print q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([], columns=['text', 'score'])\n",
    "for ix in pos.split('\\n'):\n",
    "    text = ix.strip().lower()\n",
    "    if len(text) > 1:\n",
    "        df = df.append({'text': text, 'score': 1}, ignore_index=True)\n",
    "    # print sequence_to_data(ix.strip().lower()).shape\n",
    "\n",
    "for ix in neg.split('\\n'):\n",
    "    text = ix.strip().lower()\n",
    "    if len(text) > 1:\n",
    "        df = df.append({'text': text, 'score': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "df = pd.read_csv('../data/Text/SentimentAnalysis/dataset.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for single digits kidlets stuart little 2 is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just entertaining enough not to hate , too med...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's rather like a lifetime special -- pleasan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if it's seldom boring , well , it's also rarel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  for single digits kidlets stuart little 2 is s...      0\n",
       "1  just entertaining enough not to hate , too med...      0\n",
       "2  a visually flashy but narratively opaque and e...      0\n",
       "3  it's rather like a lifetime special -- pleasan...      1\n",
       "4  if it's seldom boring , well , it's also rarel...      0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.str returns the strring val\n",
    "# .split returns the splitted string\n",
    "# .apply appllies the given function\n",
    "df['len'] = df['text'].str.split().apply(lambda x: len(x)) #return a pandas datafram coloumn\n",
    "# df = df.sort_index(ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for single digits kidlets stuart little 2 is s...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just entertaining enough not to hate , too med...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's rather like a lifetime special -- pleasan...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if it's seldom boring , well , it's also rarel...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len\n",
       "0  for single digits kidlets stuart little 2 is s...      0   33\n",
       "1  just entertaining enough not to hate , too med...      0   12\n",
       "2  a visually flashy but narratively opaque and e...      0   15\n",
       "3  it's rather like a lifetime special -- pleasan...      1   13\n",
       "4  if it's seldom boring , well , it's also rarel...      0   12"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " #df.to_csv('../data/Text/SentimentAnalysis/dataset.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make buckets of strings of diffrent lengths\n",
    "bucket_sizes = [[0, 10], [10, 15], [15, 20], [20, 25], [25, 45]]\n",
    "\n",
    "def assign_bucket(x):\n",
    "    for bucket in bucket_sizes:\n",
    "        if x > bucket[0] and x <= bucket[1]:\n",
    "            return bucket_sizes.index(bucket)\n",
    "    return len(bucket_sizes)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for single digits kidlets stuart little 2 is s...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just entertaining enough not to hate , too med...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it's rather like a lifetime special -- pleasan...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if it's seldom boring , well , it's also rarel...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len  bucket\n",
       "0  for single digits kidlets stuart little 2 is s...      0   33       4\n",
       "1  just entertaining enough not to hate , too med...      0   12       1\n",
       "2  a visually flashy but narratively opaque and e...      0   15       1\n",
       "3  it's rather like a lifetime special -- pleasan...      1   13       1\n",
       "4  if it's seldom boring , well , it's also rarel...      0   12       1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bucket'] = df.len.apply(assign_bucket)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubhijain/.local/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>incoherence reigns .</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>disney again ransacks its archives for a quick...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>its sheer dynamism is infectious .</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>snipes is both a snore and utter tripe .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>a movie with a real anarchic flair .</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  score  len  bucket\n",
       "10661                               incoherence reigns .      0    3       0\n",
       "4878   disney again ransacks its archives for a quick...      0   10       0\n",
       "10204                 its sheer dynamism is infectious .      1    6       0\n",
       "4907            snipes is both a snore and utter tripe .      0    9       0\n",
       "1543                a movie with a real anarchic flair .      1    8       0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort(columns=['bucket'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>incoherence reigns .</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>disney again ransacks its archives for a quick...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>its sheer dynamism is infectious .</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>snipes is both a snore and utter tripe .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>a movie with a real anarchic flair .</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>doesn't amount to much of anything .</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>rarely has a film's title served such dire war...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9150</th>\n",
       "      <td>there's no reason to miss interview with the a...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>it's the funniest american comedy since graffi...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>at once disarmingly straightforward and striki...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>forced , familiar and thoroughly condescending .</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>sluggish , tonally uneven .</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>collapses under its own meager weight .</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10212</th>\n",
       "      <td>there's nothing interesting in unfaithful what...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>an average kid-empowerment fantasy with slight...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>comes across as a fairly weak retooling .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>as happily glib and vicious as its characters .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10214</th>\n",
       "      <td>a sentimental mess that never rings true .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>showtime is closer to slowtime .</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>my own minority report is that it stinks .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>highly engaging .</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>an engrossing and infectiously enthusiastic do...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>efficient , suitably anonymous chiller .</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>this time kaufman's imagination has failed him .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>a movie to forget</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>morton is , as usual , brilliant .</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>barely gets off the ground .</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>an often unfunny romp .</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>high crimes miscasts nearly every leading char...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>many insightful moments .</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7879</th>\n",
       "      <td>a chiller resolutely without chills .</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6573</th>\n",
       "      <td>a beautifully observed character piece .</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6572</th>\n",
       "      <td>flat , misguided comedy .</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>a dazzling dream of a documentary .</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>turns potentially forgettable formula into som...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>life is a crock -- or something like it .</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>it's all surface psychodramatics .</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>eight legged freaks falls flat as a spoof .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>. . . too dull to enjoy .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>a mostly intelligent , engrossing and psycholo...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>pretty good little movie .</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>simultaneously heartbreakingly beautiful and e...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>a sermonizing and lifeless paean to teenage du...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7890</th>\n",
       "      <td>what's next ? the porky's revenge : ultimate e...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6206</th>\n",
       "      <td>what makes it worth watching is quaid's perfor...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>somehow both wildly implausible and strangely ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>deserving of its critical backlash and more .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>weird , vulgar comedy that's definitely an acq...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>an inexperienced director , mehta has much to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>a stirring road movie .</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>well , it does go on forever .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>the film is quiet , threatening and unforgetta...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>a sour , nasty offering .</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>drowning's too good for this sucker .</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7902</th>\n",
       "      <td>more trifle than triumph .</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>slow , silly and unintentionally hilarious .</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>the fetid underbelly of fame has never looked ...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206</th>\n",
       "      <td>[a] devastatingly powerful and astonishingly v...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>most of the action setups are incoherent .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>a fast-paced , glitzy but extremely silly piece .</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1508 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  score  len  bucket\n",
       "10661                               incoherence reigns .      0    3       0\n",
       "4878   disney again ransacks its archives for a quick...      0   10       0\n",
       "10204                 its sheer dynamism is infectious .      1    6       0\n",
       "4907            snipes is both a snore and utter tripe .      0    9       0\n",
       "1543                a movie with a real anarchic flair .      1    8       0\n",
       "1541                doesn't amount to much of anything .      0    7       0\n",
       "4917   rarely has a film's title served such dire war...      0   10       0\n",
       "9150   there's no reason to miss interview with the a...      1    9       0\n",
       "4921   it's the funniest american comedy since graffi...      0    9       0\n",
       "1534   at once disarmingly straightforward and striki...      1    8       0\n",
       "1530    forced , familiar and thoroughly condescending .      0    7       0\n",
       "4937                         sluggish , tonally uneven .      0    5       0\n",
       "9144             collapses under its own meager weight .      0    7       0\n",
       "10212  there's nothing interesting in unfaithful what...      0    7       0\n",
       "4945   an average kid-empowerment fantasy with slight...      1    9       0\n",
       "4955           comes across as a fairly weak retooling .      0    8       0\n",
       "4962     as happily glib and vicious as its characters .      0    9       0\n",
       "10214         a sentimental mess that never rings true .      0    8       0\n",
       "1518                    showtime is closer to slowtime .      0    6       0\n",
       "9130          my own minority report is that it stinks .      0    9       0\n",
       "4984                                   highly engaging .      1    3       0\n",
       "4985   an engrossing and infectiously enthusiastic do...      1    7       0\n",
       "4989            efficient , suitably anonymous chiller .      1    6       0\n",
       "9123    this time kaufman's imagination has failed him .      0    8       0\n",
       "5004                                   a movie to forget      0    4       0\n",
       "1505                  morton is , as usual , brilliant .      1    8       0\n",
       "10202                       barely gets off the ground .      0    6       0\n",
       "4877                             an often unfunny romp .      0    5       0\n",
       "4869   high crimes miscasts nearly every leading char...      0    8       0\n",
       "1557                           many insightful moments .      1    4       0\n",
       "...                                                  ...    ...  ...     ...\n",
       "7879               a chiller resolutely without chills .      0    6       0\n",
       "6573            a beautifully observed character piece .      1    6       0\n",
       "6572                           flat , misguided comedy .      0    5       0\n",
       "134                  a dazzling dream of a documentary .      1    7       0\n",
       "8633   turns potentially forgettable formula into som...      1    9       0\n",
       "125            life is a crock -- or something like it .      0   10       0\n",
       "927                   it's all surface psychodramatics .      0    5       0\n",
       "8736         eight legged freaks falls flat as a spoof .      0    9       0\n",
       "781                            . . . too dull to enjoy .      0    8       0\n",
       "131    a mostly intelligent , engrossing and psycholo...      1   10       0\n",
       "8120                          pretty good little movie .      1    5       0\n",
       "6558   simultaneously heartbreakingly beautiful and e...      1    7       0\n",
       "782    a sermonizing and lifeless paean to teenage du...      0    9       0\n",
       "7890   what's next ? the porky's revenge : ultimate e...      0   10       0\n",
       "6206   what makes it worth watching is quaid's perfor...      1    9       0\n",
       "139    somehow both wildly implausible and strangely ...      0    8       0\n",
       "8118       deserving of its critical backlash and more .      0    8       0\n",
       "7894   weird , vulgar comedy that's definitely an acq...      1   10       0\n",
       "8079   an inexperienced director , mehta has much to ...      0   10       0\n",
       "6600                             a stirring road movie .      1    5       0\n",
       "6196                      well , it does go on forever .      0    8       0\n",
       "783    the film is quiet , threatening and unforgetta...      1    9       0\n",
       "6245                           a sour , nasty offering .      0    6       0\n",
       "8208               drowning's too good for this sucker .      0    7       0\n",
       "7902                          more trifle than triumph .      0    5       0\n",
       "8138        slow , silly and unintentionally hilarious .      0    7       0\n",
       "943    the fetid underbelly of fame has never looked ...      0   10       0\n",
       "8206   [a] devastatingly powerful and astonishingly v...      1    9       0\n",
       "938           most of the action setups are incoherent .      0    8       0\n",
       "6606   a fast-paced , glitzy but extremely silly piece .      0    9       0\n",
       "\n",
       "[1508 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bucket==0 #returns true if col has 0 else false\n",
    "df[df.bucket==0] #returns the df with bucket only 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size=10, gpu=False):\n",
    "    \n",
    "    #iterartes on buckets\n",
    "    for bx in range(len(bucket_sizes)):\n",
    "        bucket_data = df[(df.bucket == bx)].reset_index(drop=True)\n",
    "        # print bx, bucket_sizes[bx][1], bucket_data.shape\n",
    "        \n",
    "        start = 0\n",
    "        stop = start + batch_size\n",
    "        \n",
    "        #iterartes on dataset\n",
    "        while start < bucket_data.shape[0]:\n",
    "            seq_length = bucket_sizes[bx][1]\n",
    "            section = bucket_data[start:stop]\n",
    "            X_data = seq_data_matrix(section.text, max_len=seq_length)\n",
    "            y_data = section.score\n",
    "            \n",
    "            if gpu:\n",
    "                yield Variable(torch.FloatTensor(X_data).cuda(), requires_grad=True), Variable(torch.LongTensor(y_data)).cuda()\n",
    "            else:\n",
    "                yield Variable(torch.FloatTensor(X_data), requires_grad=True), Variable(torch.LongTensor(y_data))\n",
    "            \n",
    "            start = stop\n",
    "            stop = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e42fd29314e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-ac4025a2dafd>\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(data, batch_size, gpu)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_data_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a20b9a0cbb4d>\u001b[0m in \u001b[0;36mseq_data_matrix\u001b[0;34m(seq_data, max_len)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseq_data_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mn_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a20b9a0cbb4d>\u001b[0m in \u001b[0;36msequence_to_data\u001b[0;34m(seq, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for ix, iy in make_batch(df, batch_size=1000,gpu=False):\n",
    "    print ix.shape, iy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(10)\n",
    "# Printing colored text (Useful later)\n",
    "# print colored(\"hello red world\", 'blue')# print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, in_shape=None, out_shape=None, hidden_shape=None):\n",
    "        super(SeqModel, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.in_shape,\n",
    "            hidden_size=self.hidden_shape,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lin = nn.Linear(self.hidden_shape, 64)\n",
    "        self.dropout = nn.Dropout(0.42)\n",
    "        self.out = nn.Linear(64, self.out_shape)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        r_out, h_state = self.rnn(x, h)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        y = F.tanh(self.lin(last_out))\n",
    "        y = self.dropout(y)\n",
    "        y = F.softmax(self.out(y))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        pred = self.forward(torch.FloatTensor(x), h_state)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        r_out, h = self.rnn(torch.FloatTensor(x), h_state)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        \n",
    "        return last_out.data.numpy()\n",
    "            \n",
    "    def init_hidden(self, batch_size, gpu=True):\n",
    "        if gpu:\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape).cuda()),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)).cuda())\n",
    "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)),\n",
    "                Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqModel(\n",
      "  (rnn): LSTM(300, 25, batch_first=True)\n",
      "  (lin): Linear(in_features=25, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.42)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (rnn): LSTM(300, 25, batch_first=True)\n",
       "  (lin): Linear(in_features=25, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.42)\n",
       "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeqModel(in_shape=300, hidden_shape=25, out_shape=2)\n",
    "\n",
    "print model\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict('hello bad world')\n",
    "\n",
    "# Load the model\n",
    "# model.load_state_dict(torch.load('/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm/model_256h_epoch_240.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.69742333889 at Epoch: 0 | Step: 0\n",
      "Loss: 0.69163531065 at Epoch: 0 | Step: 20\n",
      "Loss: 0.692053377628 at Epoch: 0 | Step: 40\n",
      "Overall Average Loss: 0.694422006607 at Epoch: 0\n",
      "Loss: 0.692090451717 at Epoch: 1 | Step: 0\n",
      "Loss: 0.687447428703 at Epoch: 1 | Step: 20\n",
      "Loss: 0.689199209213 at Epoch: 1 | Step: 40\n",
      "Overall Average Loss: 0.689712703228 at Epoch: 1\n",
      "Loss: 0.681926727295 at Epoch: 2 | Step: 0\n",
      "Loss: 0.668727934361 at Epoch: 2 | Step: 20\n",
      "Loss: 0.647350132465 at Epoch: 2 | Step: 40\n",
      "Overall Average Loss: 0.674889683723 at Epoch: 2\n",
      "Loss: 0.63601154089 at Epoch: 3 | Step: 0\n",
      "Loss: 0.608234405518 at Epoch: 3 | Step: 20\n",
      "Loss: 0.593071639538 at Epoch: 3 | Step: 40\n",
      "Overall Average Loss: 0.638657689095 at Epoch: 3\n",
      "Loss: 0.577009916306 at Epoch: 4 | Step: 0\n",
      "Loss: 0.578892111778 at Epoch: 4 | Step: 20\n",
      "Loss: 0.581489622593 at Epoch: 4 | Step: 40\n",
      "Overall Average Loss: 0.612688839436 at Epoch: 4\n",
      "Loss: 0.549013614655 at Epoch: 5 | Step: 0\n",
      "Loss: 0.570038259029 at Epoch: 5 | Step: 20\n",
      "Loss: 0.575555443764 at Epoch: 5 | Step: 40\n",
      "Overall Average Loss: 0.593836128712 at Epoch: 5\n",
      "Loss: 0.526914358139 at Epoch: 6 | Step: 0\n",
      "Loss: 0.56396317482 at Epoch: 6 | Step: 20\n",
      "Loss: 0.570190310478 at Epoch: 6 | Step: 40\n",
      "Overall Average Loss: 0.584912419319 at Epoch: 6\n",
      "Loss: 0.51433557272 at Epoch: 7 | Step: 0\n",
      "Loss: 0.557445347309 at Epoch: 7 | Step: 20\n",
      "Loss: 0.571662664413 at Epoch: 7 | Step: 40\n",
      "Overall Average Loss: 0.582174420357 at Epoch: 7\n",
      "Loss: 0.525627613068 at Epoch: 8 | Step: 0\n",
      "Loss: 0.543232917786 at Epoch: 8 | Step: 20\n",
      "Loss: 0.556832849979 at Epoch: 8 | Step: 40\n",
      "Overall Average Loss: 0.57739084959 at Epoch: 8\n",
      "Loss: 0.525994777679 at Epoch: 9 | Step: 0\n",
      "Loss: 0.545738399029 at Epoch: 9 | Step: 20\n",
      "Loss: 0.544856071472 at Epoch: 9 | Step: 40\n",
      "Overall Average Loss: 0.570654571056 at Epoch: 9\n",
      "Loss: 0.513692259789 at Epoch: 10 | Step: 0\n",
      "Loss: 0.541399240494 at Epoch: 10 | Step: 20\n",
      "Loss: 0.542103528976 at Epoch: 10 | Step: 40\n",
      "Overall Average Loss: 0.563399195671 at Epoch: 10\n",
      "Loss: 0.505694925785 at Epoch: 11 | Step: 0\n",
      "Loss: 0.533120155334 at Epoch: 11 | Step: 20\n",
      "Loss: 0.536459207535 at Epoch: 11 | Step: 40\n",
      "Overall Average Loss: 0.557379901409 at Epoch: 11\n",
      "Loss: 0.493396878242 at Epoch: 12 | Step: 0\n",
      "Loss: 0.526925086975 at Epoch: 12 | Step: 20\n",
      "Loss: 0.531074225903 at Epoch: 12 | Step: 40\n",
      "Overall Average Loss: 0.551092982292 at Epoch: 12\n",
      "Loss: 0.489806681871 at Epoch: 13 | Step: 0\n",
      "Loss: 0.519273877144 at Epoch: 13 | Step: 20\n",
      "Loss: 0.519293963909 at Epoch: 13 | Step: 40\n",
      "Overall Average Loss: 0.545694589615 at Epoch: 13\n",
      "Loss: 0.48345375061 at Epoch: 14 | Step: 0\n",
      "Loss: 0.51608812809 at Epoch: 14 | Step: 20\n",
      "Loss: 0.513243317604 at Epoch: 14 | Step: 40\n",
      "Overall Average Loss: 0.541397333145 at Epoch: 14\n",
      "Loss: 0.481462657452 at Epoch: 15 | Step: 0\n",
      "Loss: 0.510341882706 at Epoch: 15 | Step: 20\n",
      "Loss: 0.511853575706 at Epoch: 15 | Step: 40\n",
      "Overall Average Loss: 0.537546873093 at Epoch: 15\n",
      "Loss: 0.480911433697 at Epoch: 16 | Step: 0\n",
      "Loss: 0.513807117939 at Epoch: 16 | Step: 20\n",
      "Loss: 0.509938836098 at Epoch: 16 | Step: 40\n",
      "Overall Average Loss: 0.534784376621 at Epoch: 16\n",
      "Loss: 0.47408130765 at Epoch: 17 | Step: 0\n",
      "Loss: 0.50886631012 at Epoch: 17 | Step: 20\n",
      "Loss: 0.510581076145 at Epoch: 17 | Step: 40\n",
      "Overall Average Loss: 0.530485510826 at Epoch: 17\n",
      "Loss: 0.470617681742 at Epoch: 18 | Step: 0\n",
      "Loss: 0.505066335201 at Epoch: 18 | Step: 20\n",
      "Loss: 0.51499325037 at Epoch: 18 | Step: 40\n",
      "Overall Average Loss: 0.52730178833 at Epoch: 18\n",
      "Loss: 0.46736240387 at Epoch: 19 | Step: 0\n",
      "Loss: 0.505021572113 at Epoch: 19 | Step: 20\n",
      "Loss: 0.512701034546 at Epoch: 19 | Step: 40\n",
      "Overall Average Loss: 0.524192869663 at Epoch: 19\n",
      "Loss: 0.470387965441 at Epoch: 20 | Step: 0\n",
      "Loss: 0.504788458347 at Epoch: 20 | Step: 20\n",
      "Loss: 0.508899629116 at Epoch: 20 | Step: 40\n",
      "Overall Average Loss: 0.523307919502 at Epoch: 20\n",
      "Loss: 0.464465528727 at Epoch: 21 | Step: 0\n",
      "Loss: 0.507555365562 at Epoch: 21 | Step: 20\n",
      "Loss: 0.515251517296 at Epoch: 21 | Step: 40\n",
      "Overall Average Loss: 0.52463054657 at Epoch: 21\n",
      "Loss: 0.469170838594 at Epoch: 22 | Step: 0\n",
      "Loss: 0.501567602158 at Epoch: 22 | Step: 20\n",
      "Loss: 0.50456982851 at Epoch: 22 | Step: 40\n",
      "Overall Average Loss: 0.526973605156 at Epoch: 22\n",
      "Loss: 0.485012233257 at Epoch: 23 | Step: 0\n",
      "Loss: 0.498085379601 at Epoch: 23 | Step: 20\n",
      "Loss: 0.495019763708 at Epoch: 23 | Step: 40\n",
      "Overall Average Loss: 0.523140490055 at Epoch: 23\n",
      "Loss: 0.466693073511 at Epoch: 24 | Step: 0\n",
      "Loss: 0.485822230577 at Epoch: 24 | Step: 20\n",
      "Loss: 0.494245409966 at Epoch: 24 | Step: 40\n",
      "Overall Average Loss: 0.514452755451 at Epoch: 24\n",
      "Loss: 0.471115678549 at Epoch: 25 | Step: 0\n",
      "Loss: 0.483766198158 at Epoch: 25 | Step: 20\n",
      "Loss: 0.489568740129 at Epoch: 25 | Step: 40\n",
      "Overall Average Loss: 0.512136101723 at Epoch: 25\n",
      "Loss: 0.474546521902 at Epoch: 26 | Step: 0\n",
      "Loss: 0.48164254427 at Epoch: 26 | Step: 20\n",
      "Loss: 0.486355632544 at Epoch: 26 | Step: 40\n",
      "Overall Average Loss: 0.511702656746 at Epoch: 26\n",
      "Loss: 0.477392196655 at Epoch: 27 | Step: 0\n",
      "Loss: 0.495191037655 at Epoch: 27 | Step: 20\n",
      "Loss: 0.496343642473 at Epoch: 27 | Step: 40\n",
      "Overall Average Loss: 0.518317103386 at Epoch: 27\n",
      "Loss: 0.456563651562 at Epoch: 28 | Step: 0\n",
      "Loss: 0.484345287085 at Epoch: 28 | Step: 20\n",
      "Loss: 0.490119546652 at Epoch: 28 | Step: 40\n",
      "Overall Average Loss: 0.513072371483 at Epoch: 28\n",
      "Loss: 0.451302081347 at Epoch: 29 | Step: 0\n",
      "Loss: 0.479845076799 at Epoch: 29 | Step: 20\n",
      "Loss: 0.487130314112 at Epoch: 29 | Step: 40\n",
      "Overall Average Loss: 0.506174564362 at Epoch: 29\n",
      "Loss: 0.451211124659 at Epoch: 30 | Step: 0\n",
      "Loss: 0.475132018328 at Epoch: 30 | Step: 20\n",
      "Loss: 0.484574496746 at Epoch: 30 | Step: 40\n",
      "Overall Average Loss: 0.502227902412 at Epoch: 30\n",
      "Loss: 0.451631844044 at Epoch: 31 | Step: 0\n",
      "Loss: 0.475101321936 at Epoch: 31 | Step: 20\n",
      "Loss: 0.487421482801 at Epoch: 31 | Step: 40\n",
      "Overall Average Loss: 0.498895674944 at Epoch: 31\n",
      "Loss: 0.447932511568 at Epoch: 32 | Step: 0\n",
      "Loss: 0.474938631058 at Epoch: 32 | Step: 20\n",
      "Loss: 0.484663814306 at Epoch: 32 | Step: 40\n",
      "Overall Average Loss: 0.497542649508 at Epoch: 32\n",
      "Loss: 0.444742619991 at Epoch: 33 | Step: 0\n",
      "Loss: 0.471693068743 at Epoch: 33 | Step: 20\n",
      "Loss: 0.484110414982 at Epoch: 33 | Step: 40\n",
      "Overall Average Loss: 0.493412494659 at Epoch: 33\n",
      "Loss: 0.442980676889 at Epoch: 34 | Step: 0\n",
      "Loss: 0.467999815941 at Epoch: 34 | Step: 20\n",
      "Loss: 0.478754997253 at Epoch: 34 | Step: 40\n",
      "Overall Average Loss: 0.492076158524 at Epoch: 34\n",
      "Loss: 0.440233498812 at Epoch: 35 | Step: 0\n",
      "Loss: 0.470312267542 at Epoch: 35 | Step: 20\n",
      "Loss: 0.476249724627 at Epoch: 35 | Step: 40\n",
      "Overall Average Loss: 0.489179372787 at Epoch: 35\n",
      "Loss: 0.439033806324 at Epoch: 36 | Step: 0\n",
      "Loss: 0.469206631184 at Epoch: 36 | Step: 20\n",
      "Loss: 0.475227326155 at Epoch: 36 | Step: 40\n",
      "Overall Average Loss: 0.49189427495 at Epoch: 36\n",
      "Loss: 0.436659693718 at Epoch: 37 | Step: 0\n",
      "Loss: 0.491647422314 at Epoch: 37 | Step: 20\n",
      "Loss: 0.472197830677 at Epoch: 37 | Step: 40\n",
      "Overall Average Loss: 0.505670905113 at Epoch: 37\n",
      "Loss: 0.457237482071 at Epoch: 38 | Step: 0\n",
      "Loss: 0.466650813818 at Epoch: 38 | Step: 20\n",
      "Loss: 0.481216132641 at Epoch: 38 | Step: 40\n",
      "Overall Average Loss: 0.500137031078 at Epoch: 38\n",
      "Loss: 0.47481316328 at Epoch: 39 | Step: 0\n",
      "Loss: 0.467495381832 at Epoch: 39 | Step: 20\n",
      "Loss: 0.470978289843 at Epoch: 39 | Step: 40\n",
      "Overall Average Loss: 0.496206909418 at Epoch: 39\n",
      "Loss: 0.461064308882 at Epoch: 40 | Step: 0\n",
      "Loss: 0.464761227369 at Epoch: 40 | Step: 20\n",
      "Loss: 0.46520626545 at Epoch: 40 | Step: 40\n",
      "Overall Average Loss: 0.491576761007 at Epoch: 40\n",
      "Loss: 0.469571202993 at Epoch: 41 | Step: 0\n",
      "Loss: 0.459188580513 at Epoch: 41 | Step: 20\n",
      "Loss: 0.462537109852 at Epoch: 41 | Step: 40\n",
      "Overall Average Loss: 0.486786901951 at Epoch: 41\n",
      "Loss: 0.469054400921 at Epoch: 42 | Step: 0\n",
      "Loss: 0.459449231625 at Epoch: 42 | Step: 20\n",
      "Loss: 0.463728100061 at Epoch: 42 | Step: 40\n",
      "Overall Average Loss: 0.484697341919 at Epoch: 42\n",
      "Loss: 0.457308411598 at Epoch: 43 | Step: 0\n",
      "Loss: 0.455283880234 at Epoch: 43 | Step: 20\n",
      "Loss: 0.461940914392 at Epoch: 43 | Step: 40\n",
      "Overall Average Loss: 0.485280305147 at Epoch: 43\n",
      "Loss: 0.457079768181 at Epoch: 44 | Step: 0\n",
      "Loss: 0.451515763998 at Epoch: 44 | Step: 20\n",
      "Loss: 0.457645446062 at Epoch: 44 | Step: 40\n",
      "Overall Average Loss: 0.482401043177 at Epoch: 44\n",
      "Loss: 0.455950021744 at Epoch: 45 | Step: 0\n",
      "Loss: 0.45035892725 at Epoch: 45 | Step: 20\n",
      "Loss: 0.46109944582 at Epoch: 45 | Step: 40\n",
      "Overall Average Loss: 0.482721477747 at Epoch: 45\n",
      "Loss: 0.450999975204 at Epoch: 46 | Step: 0\n",
      "Loss: 0.457892537117 at Epoch: 46 | Step: 20\n",
      "Loss: 0.473802834749 at Epoch: 46 | Step: 40\n",
      "Overall Average Loss: 0.484114319086 at Epoch: 46\n",
      "Loss: 0.439380735159 at Epoch: 47 | Step: 0\n",
      "Loss: 0.456071287394 at Epoch: 47 | Step: 20\n",
      "Loss: 0.470109641552 at Epoch: 47 | Step: 40\n",
      "Overall Average Loss: 0.479265272617 at Epoch: 47\n",
      "Loss: 0.43844383955 at Epoch: 48 | Step: 0\n",
      "Loss: 0.446683079004 at Epoch: 48 | Step: 20\n",
      "Loss: 0.470870375633 at Epoch: 48 | Step: 40\n",
      "Overall Average Loss: 0.478016227484 at Epoch: 48\n",
      "Loss: 0.448094546795 at Epoch: 49 | Step: 0\n",
      "Loss: 0.448546677828 at Epoch: 49 | Step: 20\n",
      "Loss: 0.494729071856 at Epoch: 49 | Step: 40\n",
      "Overall Average Loss: 0.481781721115 at Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# Set to train mode\n",
    "# model.cuda()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    N = 0\n",
    "    for step, (b_x, b_y) in enumerate(make_batch(df, batch_size=200)):\n",
    "        # print step, b_x.shape, b_y.shape\n",
    "        bsize = b_x.size(0)\n",
    "        lol = b_x\n",
    "        h_state = model.init_hidden(bsize, gpu=True)\n",
    "\n",
    "        pred = model(b_x, h_state)\n",
    "        loss = criterion(pred, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        N += 1.0\n",
    "        if step%20 == 0:\n",
    "            print 'Loss: {} at Epoch: {} | Step: {}'.format(loss, epoch, step)\n",
    "        \n",
    "    print \"Overall Average Loss: {} at Epoch: {}\".format(total_loss / float(N), epoch)\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), \"/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm_bucket/model_256h_epoch_{}.ckpt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25) (1, 25)\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "v1 = model.get_embedding('I am going to a place')\n",
    "v2 = model.get_embedding('I am not going')\n",
    "print v1.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4187516]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.pairwise.cosine_distances(v1, v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
